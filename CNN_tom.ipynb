{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "Erstmal nur einfache Unterscheidung in Roboter / Ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras import models, optimizers, losses, activations\n",
    "from tensorflow.python.keras.layers import *\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.15.3\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Data\n",
    "\n",
    "# force reload images from HUBOX\n",
    "FORCE = False\n",
    "\n",
    "# validation split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# folder path to image files (relatative)\n",
    "IMAGE_FOLDER_NAME = 'images/robot_ball_dataset'\n",
    "\n",
    "# image size\n",
    "DIMEN = 240\n",
    "\n",
    "# batch size ImageDataGenerator\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "## Load Data\n",
    "\n",
    "# path for npy files\n",
    "DATA_FOLDER_PATH = 'images/processed_data'\n",
    "\n",
    "\n",
    "## Prepare Tensorflow Parameter\n",
    "\n",
    "NUMBER_OF_CLASSES = 2\n",
    "EPOCHS = 10\n",
    "DROPOUT_RATE = 0.25\n",
    "\n",
    "MODEL_PATH = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data from HU Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images files from HU Box\n",
    "\n",
    "if FORCE or not os.path.exists(\"images\"):\n",
    "    print('TODO download files')\n",
    "    # !./get_images.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9332 images belonging to 2 classes.\n",
      "Found 2332 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255, \n",
    "    validation_split = VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory( \n",
    "    IMAGE_FOLDER_NAME, \n",
    "    target_size = (DIMEN, DIMEN), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    class_mode = 'binary',\n",
    "    subset = 'training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory( \n",
    "    IMAGE_FOLDER_NAME, \n",
    "    target_size = (DIMEN, DIMEN), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    class_mode = 'binary',\n",
    "    subset = 'validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (16, 240, 240, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5834c7c24aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5613\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5615\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5616\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5617\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/PyCharmProjects/3-object-detection/venv/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (16, 240, 240, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACICAYAAAA8uqNSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAABmUlEQVR4nO3VsW1CUQxA0f+ijAB1/v6zwBDUyQ7OAnAFUhBIOae1CxdX8pqZDW75ePUBvDeBkARCEghJICSBkD4fWT4cDrPv+5NO4VXO5/PPzByvzR4KZN/37XQ6/c1VvI211uXWzIshCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASAIhCYQkEJJASGtm7l9e63vbtsvzzuFFvmbmeG3wUCD8P14MSSAkgZAEQhIISSAkgZAEQhII6Re9LBqx3STodAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i in range(40):\n",
    "    plt.subplot(5, 8, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_generator[i][0].squeeze(), cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensorflow Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = activations.relu\n",
    "\n",
    "NEURAL_SCHEMA = [\n",
    "\n",
    "    Conv2D(32, (3, 3), activation=activation_func, input_shape=(240, 240, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation=activation_func),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(100, activation=activation_func),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "\n",
    "    Dense(NUMBER_OF_CLASSES, activation=tf.nn.softmax)\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(NEURAL_SCHEMA)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.2221 - acc: 0.9531Epoch 1/10\n",
      "187/187 [==============================] - 291s 2s/step - loss: 0.2209 - acc: 0.9534 - val_loss: 0.0063 - val_acc: 0.9974\n",
      "Epoch 2/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0249 - acc: 0.9922Epoch 1/10\n",
      "187/187 [==============================] - 285s 2s/step - loss: 0.0248 - acc: 0.9923 - val_loss: 0.0030 - val_acc: 0.9991\n",
      "Epoch 3/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0456 - acc: 0.9892Epoch 1/10\n",
      "187/187 [==============================] - 289s 2s/step - loss: 0.0464 - acc: 0.9890 - val_loss: 0.7066 - val_acc: 0.7243\n",
      "Epoch 4/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0436 - acc: 0.9861Epoch 1/10\n",
      "187/187 [==============================] - 285s 2s/step - loss: 0.0435 - acc: 0.9861 - val_loss: 0.0110 - val_acc: 0.9979\n",
      "Epoch 5/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0117 - acc: 0.9958Epoch 1/10\n",
      "187/187 [==============================] - 283s 2s/step - loss: 0.0116 - acc: 0.9958 - val_loss: 0.0028 - val_acc: 0.9991\n",
      "Epoch 6/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0162 - acc: 0.9941Epoch 1/10\n",
      "187/187 [==============================] - 284s 2s/step - loss: 0.0161 - acc: 0.9941 - val_loss: 0.0012 - val_acc: 0.9991\n",
      "Epoch 7/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0122 - acc: 0.9970Epoch 1/10\n",
      "187/187 [==============================] - 279s 1s/step - loss: 0.0121 - acc: 0.9970 - val_loss: 9.3230e-04 - val_acc: 0.9991\n",
      "Epoch 8/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0073 - acc: 0.9973Epoch 1/10\n",
      "187/187 [==============================] - 280s 1s/step - loss: 0.0074 - acc: 0.9972 - val_loss: 0.0018 - val_acc: 0.9991\n",
      "Epoch 9/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0042 - acc: 0.9982Epoch 1/10\n",
      "187/187 [==============================] - 280s 1s/step - loss: 0.0042 - acc: 0.9982 - val_loss: 0.0015 - val_acc: 0.9996\n",
      "Epoch 10/10\n",
      "186/187 [============================>.] - ETA: 1s - loss: 0.0035 - acc: 0.9987Epoch 1/10\n",
      "187/187 [==============================] - 287s 2s/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0022 - val_acc: 0.9991\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 238, 238, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 119, 119, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 117, 117, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 58, 58, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 215296)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               21529700  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 21,549,294\n",
      "Trainable params: 21,549,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Elapsed time acquired for 10 epoch(s) -> 47.380865800380704 minutes\n"
     ]
    }
   ],
   "source": [
    "initial_time = time.time()\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    #steps_per_epoch = train_generator.samples // 16,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = validation_generator, \n",
    "    #validation_steps = validation_generator.samples // 16,\n",
    ")\n",
    "    \n",
    "final_time = time.time()\n",
    "eta = (final_time - initial_time)\n",
    "time_unit = 'seconds'\n",
    "if eta >= 60:\n",
    "    eta = eta / 60\n",
    "    time_unit = 'minutes'\n",
    "model.summary()\n",
    "print('Elapsed time acquired for {} epoch(s) -> {} {}'.format(EPOCHS, eta, time_unit))\n",
    "\n",
    "model_path = os.path.join(MODEL_PATH, 'model-{}.h5'.format(datetime.fromtimestamp(final_time).strftime(\"%Y-%m-%d_%H-%M\")))\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
